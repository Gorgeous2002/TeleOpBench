<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation">
  <meta name="keywords" content="Humanoid, Loco-Manipulation, Teleoperation, Robot Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation
  </title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QZ38WT2YPD"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-QZ38WT2YPD');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./statics/css/bulma.min.css">
  <link rel="stylesheet" href="./statics/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./statics/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./statics/css/index.css">

  <script src="https://kit.fontawesome.com/19914a84eb.js" crossorigin="anonymous"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./statics/js/bulma-carousel.min.js"></script>
  <script src="./statics/js/bulma-slider.min.js"></script>
  <script src="./statics/js/index.js"></script>
</head>

<section class="hero is-link is-fullheight video" style="overflow: hidden; position:relative;">
  <div class="hero-video" style="height: 100%; width: 177.77777778vh; min-width: 100%;min-height: 56.25vw;">
    <video playsinline autoplay muted loop>
      <source src=" ./statics/videolq/Simple.mp4" type="video/mp4">
      <!-- 封面视频 TO ADD -->
    </video>
  </div>
  <div class="hero-video is-hidden-tablet is-inline-block-mobile"
    style="height: 154.28571428vw; width: 100%; min-width:64.81481481vh;min-height:100%;">
    <video playsinline autoplay muted loop>
      <source src=" ./statics/videolq/Simple.mp4" type="video/mp4">
      <!-- 封面视频 TO ADD -->
    </video>
  </div>
  <div class="overlay"></div>
  <!-- Hero head: will stick at the top -->
  <div class="hero-head is-hidden-mobile">
    <header class="navbar">
      <div class="container is-size-5">
        <div class="navbar-menu">
          <div class="navbar-end">
            <a class="navbar-item pl-4 pr-4" href="./statics/TeleOpBench_prerint.pdf">
              <span class="icon" style="margin-right:5px;">
                <img src="./statics/images/pdf.svg" alt="PDF" />
              </span>
              <span>Paper</span>
            </a>
            <a class="navbar-item  pl-4 pr-4" href="https://arxiv.org/abs/2502.13013">
              <span class="icon" style="margin-right:5px;">
                <img src="./statics/images/arxiv.svg" alt="ArXiv" />
              </span>
              <span>arXiv</span> </a>
            <a href="https://youtu.be/PbimvvNCIdc" class="navbar-item  pl-4 pr-4">
              <span class="icon" style="margin-right:5px;">
                <img src="./statics/images/youtube.svg" alt="Youtube" />
              </span>
              <span>Video</span> </a>
            <span class="navbar-item  pl-4 pr-4">
              <a href="https://gorgeous2002.github.io/TeleOpBench/" class="button is-inverted is-large">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </header>
  </div>

  <!-- Hero content: will be in the middle -->
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1 publication-title is-size-1-mobile" style="font-size: 12rem;">
        TeleOpBench
      </h1>
      <h1 class="subtitle is-1 publication-title is-size-4-mobile" style="font-size: 4rem;">
        A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation
      </h1>
      <!-- <h1 class="is-2 is-italic is-size-4-mobile" style="font-size: 2rem; opacity: 80%;">
        Conference on Robot Learning 2024
      </h1>
      <h1 class="is-2 is-italic is-size-4-mobile" style="font-size: 1.5rem; opacity: 80%;">
        <b>Spotlight</b> WCBM Workshop CoRL 2024
      </h1> -->
      <!-- <h1 class="is-2 is-italic is-size-4-mobile" style="font-size: 1.5rem; opacity: 80%;">
        X-Embodiment Workshop CoRL 2024
      </h1> -->
      <div class="column has-text-centered is-hidden-tablet">
        <div class="publication-links">
          <!-- PDF Link. -->
          <span class="link-block">
            <a href="./statics/TeleOpBench_prerint.pdf" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <img src="./statics/images/pdf.svg" alt="q j" />
              </span>
              <span>Paper</span>
            </a>
          </span>
          <span class="link-block">
            <a href="https://arxiv.org/abs/2502.13013" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <img src="./statics/images/arxiv.svg" alt="ArXiv" />
              </span>
              <span>arXiv</span>
            </a>
          </span>
          <!-- Video Link. -->
          <span class="link-block">
            <a href="https://youtu.be/PbimvvNCIdc" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <img src="./statics/images/youtube.svg" alt="Youtube" />
              </span>
              <span>Video</span>
            </a>
          </span>
          <!-- Code Link. -->
          <span class="link-block">
            <a href="https://gorgeous2002.github.io/TeleOpBench/"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Hero footer: will stick at the bottom -->
  <div class="hero-foot is-hidden-mobile">
    <nav class="tabs is-boxed is-fullwidth is-size-5">
      <ul>
        <li><a href="#Overview">TeleOpBench</a></li>
        <li><a href="#Task Environments">Task Environments</a></li>
        <li><a href="#Teleoperation">Teleoperation Interface</a></li>
        <li><a href="#Experiments">Experiments</a></li>
      </ul>
    </nav>
  </div>
</section>

<section class="section is-medium" id="overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-justified is-hidden-mobile">
        <h2 class="title is-2 has-text-centered"
          style="color: var(--title-color, #126BAE); font-size: 3.5rem; font-weight: bold; letter-spacing: 1px; line-height: 1.2;">
          TeleOpBench:
        </h2>

        <h3 class="subtitle is-4 has-text-centered"
          style="color: var(--subtitle-color, #126BAE); font-size: 2.5rem; font-weight: 600; letter-spacing: 0.5px; line-height: 1.4;">
          A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation
        </h3>
        <div style="text-align: center; margin-bottom: 20px;">
          <img src="./statics/images/newpipe.jpg" alt="Image description" style="max-width: 100%; height: auto;">
        </div>
        <p>
          <strong>Abstract:</strong>
          Teleoperation is a cornerstone of embodied-robot learning, and bimanual dexterous teleoperation in particular
          provides rich demonstrations that are difficult to obtain with fully autonomous systems. While recent studies
          have proposed diverse hardware pipelines—ranging from inertial motion-capture gloves to exoskeletons and
          vision-based interfaces—there is still no unified benchmark that enables fair, reproducible comparison of
          these systems. In this paper, we introduce TeleOpBench, a simulator-centric benchmark tailored to bimanual
          dexterous teleoperation. TeleOpBench contains 30 high-fidelity task environments that span pick-and-place,
          tool use, and collaborative manipulation, covering a broad spectrum of kinematic and force-interaction
          difficulty. Within this benchmark we implement four representative teleoperation modalities—(i) MoCap, (ii) VR
          device, (iii) arm-hand exoskeletons, and (iv) monocular vision tracking—and evaluate them with a common
          protocol and metric suite. To validate that performance in simulation is predictive of real-world behavior, we
          conduct mirrored experiments on a physical dual-arm platform equipped with two 6-DoF dexterous hands. Across
          10 held-out tasks we observe a strong correlation between simulator and hardware performance, confirming the
          external validity of TeleOpBench. TeleOpBench establishes a common yardstick for teleoperation research and
          provides an extensible platform for future algorithmic and hardware innovation.
        </p>
      </div>

      <div class="column is-four-fifths is-hidden-tablet has-text-justified-mobile">
        <h2 class="title is-2 has-text-centered"
          style="color: var(--title-color, #126BAE); font-size: 3.5rem; font-weight: bold; letter-spacing: 1px; line-height: 1.2;">
          TeleOpBench:
        </h2>

        <h3 class="subtitle is-4 has-text-centered"
          style="color: var(--subtitle-color, #126BAE); font-size: 2.5rem; font-weight: 600; letter-spacing: 0.5px; line-height: 1.4;">
          A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation
        </h3>
        <div style="text-align: center; margin-bottom: 20px;">
          <img src="./statics/images/newpipe.jpg" alt="Image description" style="max-width: 100%; height: auto;">
        </div>
        <p>
          <strong>Abstract:</strong>
          Teleoperation is a cornerstone of embodied-robot learning, and bimanual dexterous teleoperation in particular
          provides rich demonstrations that are difficult to obtain with fully autonomous systems. While recent studies
          have proposed diverse hardware pipelines—ranging from inertial motion-capture gloves to exoskeletons and
          vision-based interfaces—there is still no unified benchmark that enables fair, reproducible comparison of
          these systems. In this paper, we introduce TeleOpBench, a simulator-centric benchmark tailored to bimanual
          dexterous teleoperation. TeleOpBench contains 30 high-fidelity task environments that span pick-and-place,
          tool use, and collaborative manipulation, covering a broad spectrum of kinematic and force-interaction
          difficulty. Within this benchmark we implement four representative teleoperation modalities—(i) MoCap, (ii) VR
          device, (iii) arm-hand exoskeletons, and (iv) monocular vision tracking—and evaluate them with a common
          protocol and metric suite. To validate that performance in simulation is predictive of real-world behavior, we
          conduct mirrored experiments on a physical dual-arm platform equipped with two 6-DoF dexterous hands. Across
          10 held-out tasks we observe a strong correlation between simulator and hardware performance, confirming the
          external validity of TeleOpBench. TeleOpBench establishes a common yardstick for teleoperation research and
          provides an extensible platform for future algorithmic and hardware innovation.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen" style="margin-top: -5em;">
    <hr width="100%" size="2">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/PbimvvNCIdc?rel=0&amp;showinfo=0" frameborder="0"
            allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
</div>
<section class="section is-small" id="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-2 has-text-centered is-size-4-mobile">TeleOpBench</h2>


        <div style="text-align: center;">
          <img alt="teaser" src="./statics/images/teaser.png" width="100%" />
        </div>
        <div class="content">
          <p>
            We present <strong>TeleOpBench</strong>, a simulation-based benchmark for bimanual dexterous teleoperation,
            and evaluate four representative teleoperation modalities across multiple robot platforms (row 1).
            Real-robot experiments (row 2) demonstrate four teleoperation capabilities. Our teleoperation pipelines
            support fine-precision manipulation in the real world—for example, the left hand grasps a block while the
            right hand simultaneously inserts a smaller block (row 3)—and can execute long-horizon sequences, such as
            retrieving a tomato-laden plate from a microwave with the right hand and transferring the tomatoes to a
            table with the left (rows 4 and 5). </p>
          <p>
            This paper makes the following <strong>contributions</strong>:
          </p>
          <ol>
            <li style="color: #126BAE;">
              <strong>We introduce a dedicated benchmark, TeleOpBench, for dual-arm dexterous teleoperation, enabling
                rigorous, fair, and comprehensive comparisons across competing systems.</strong>
            </li>
            <li style="color: #126BAE;">
              <strong>We implement four representative teleoperation pipelines—motion-capture, VR controllers,
                upper-body exoskeletons, and vision-only within a single modular framework.</strong>
            </li>
            <li style="color: #126BAE;">
              <strong>Extensive experiments on both TeleOpBench and a real dual-arm platform reveal a strong correlation
                between simulated and physical performance, substantiating the benchmark's fidelity and practical
                value.</strong>
            </li>
          </ol>

          <!-- 三个机器人视频展示 -->
          <div class="video-container" style="display: flex; gap: 10px;">
            <!-- G1 视频 -->
            <div style="flex: 1; text-align: center;">
              <video autoplay muted loop playsinline controls playsinline style="width: 100%; border-radius: 8px;">
                <source src="./statics/videolq/G1.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p>Unitree G1</p>
            </div>

            <!-- GR1-T2 视频 -->
            <div style="flex: 1; text-align: center;">
              <video autoplay muted loop playsinline controls playsinline style="width: 100%; border-radius: 8px;">
                <source src="./statics/videolq/Gr1t2.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p>Fourier GR1-T2</p>
            </div>

            <!-- H1-2 视频 -->
            <div style="flex: 1; text-align: center;">
              <video autoplay muted loop playsinline controls playsinline style="width: 100%; border-radius: 8px;">
                <source src="./statics/videolq/H1-2.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p>Fourier H1-2</p>
            </div>
          </div>

          <!-- 视频 and 图表
          <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 100%;">
                <source src="./statics/videolq/g1.mp4" type="video/mp4">
                Unitree G1 trained in Isaac Gym.
              </video>
              <p>Unitree G1 trained in Isaac Gym.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 100%;">
                <source src="./statics/videolq/gr1.mp4" type="video/mp4">
                Fourier GR-1 trained in Isaac Gym.
              </video>
              <p>Fourier GR-1 trained in Isaac Gym.</p>
            </div>
          </div>
          <br>
          <p>After training with our framework on an Nvidia RTX 4090 for only about <span style="color: #e9890c;">3
              hours</span> , we can get policies that can be deployed directly in the real world to drive robots walk
            and squat robustly.</p>
          <div style="text-align: center;">
            <img alt="rl" src="./statics/images/aba.png" width="100%" />
          </div> -->
          <!-- 视频 and 图表 ended -->

          <!-- <p>We conduct serval ablation experiments to verify the effectiveness of our framework, and we find:</p>
          <ol>
            <li>
              Our upper-body pose curriculum can help robots better learn to balance under dynamic upper-body movements
              gradually than methods without curriculum or with other curriculum style.
            </li>
            <li>
              The introduction of novel height tracking reward can accelerate the training for robot squatting.
            </li>
            <li>
              The symmetry utilization can both significantly accelerate the training process by over 10 times and
              guarantee the symmetry of the trained policy.
            </li> -->
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section is-small" id="Task Environments">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered is-size-4-mobile">Task Environments</h2>
        <div class="content">
          <p>We employ NVIDIA Isaac Sim as our simulation platform because its high-performance PhysX engine and
            photorealistic renderer enable the construction of environments that closely approximate real-world
            conditions. Each scene features a humanoid robot fitted with bimanual dexterous hands and the task-relevant
            objects; operators are instructed to execute the required manipulations exactly as specified. For every
            trial, we record both task success and completion time, which together constitute our primary performance
            metrics.</p>
          <!-- image and videos -->
          <!-- <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 100%; height: 100%;">
                <source src="./statics/videolq/exo.mp4" type="video/mp4">
                Arms and Hands.
              </video>
              <p>Arms and Hands.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 100%; height: 100%;">
                <source src="./statics/videolq/pedal.mp4" type="video/mp4">
                Pedal.
              </video>
              <p>Pedal.</p>
            </div>
          </div> 闭合 video-container -->
          <!-- </div> -->
          <!-- 标题 -->
          <h2 style="text-align: center; margin-top: 40px;">30 High-Fidelity Task Environments</h2>

          <!-- 展示任务图像 -->
          <div style="text-align: center; margin-top: 20px;">
            <img src="./statics/teleop_images/task_visualization.jpg" alt="Task Visualization"
              style="width: 80%; max-width: 1000px; border-radius: 8px;">
          </div>

          <!-- <div class="content">
          <p>We design hardware systems for both <span style="color: #126BAE;">Unitree G1</span> and <span
              style="color: #126BAE;">Fourier GR-1</span>. Notably, our gloves can be detached from the arms, allowing
            them to be reused in systems isomorphic to different robots. </p>
          <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <img alt="rl" src="./statics/images/g1real.png" width="50%" />
              <p><span style="color: #126BAE;">1. </span>Isomorphic Exoskeleton for Unitree G1.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <img alt="rl" src="./statics/images/gr1real.png" width="50%" />
              <p><span style="color: #126BAE;">2. </span>Isomorphic Exoskeleton for Fourier GR-1.</p>
            </div>
          </div> <!-- 闭合 video-container -->
          <!-- </div> -->
          <!-- <div class="content">
          <p>Using our hardware system, <span style="color: #e9890c;">one single operator</span> can choose to control:
          </p>

          <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 100%; height: 100%;">
                <source src="./statics/videolq/hand.mp4" type="video/mp4">
                Diverse dexterous hands.
              </video>
              <p><span style="color: #126BAE;">1. </span>Diverse dexterous hands.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 100%; height: 100%;">
                <source src="./statics/videolq/arm.mp4" type="video/mp4">
                Upper-body of Humanoids.
              </video>
              <p><span style="color: #126BAE;">2. </span>Upper body of Humanoids.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 100%; height: 100%;">
                <source src="./statics/videolq/whole.mp4" type="video/mp4">
                Whole body of humanoids.
              </video>
              <p><span style="color: #126BAE;">3. </span>Whole body of humanoids.</p>
            </div>
          </div> <!-- 闭合 video-container -->
          <!-- </div>
        <br>
        <p>The total cost of the hardware system is only <span style="color: #e9890c;">$0.5k</span>, significantly lower
          than that of MoCap devices. We list the detailed costs for all parts <a
            href="https://docs.google.com/document/d/1o9QVqU8puq1ob-knYmQQiqrZheUKpCePHz2J3zQot20">here</a>.</p>
        <div class="content"> -->
          <!-- <p>
          <h3 class="title is-3 has-text-centered is-size-4-mobile">1.Humanoid robots</h3>
          </p>
          <p>
            For a comprehensive hardware evaluation, we employ three commercially available humanoid platforms—<span
              style="color: #126BAE;">Unitree G1</span>, <span style="color: #126BAE;">Unitree H1-2</span>, and <span
              style="color: #126BAE;">Fourier GR1-T2</span>.
          </p>
          <p>
          <h3 class="title is-3 has-text-centered is-size-4-mobile">2.Task setting</h3>
          </p>
          <p>
            TeleOpBench offers a comprehensive, multi-tiered suite of <span style="color: #e9890c;">30</span> bimanual
            dexterousmanipulation tasks. The tasks
            are hierarchically organized by complexity—e.g., whether they require coordinated two-hand interaction or
            long-horizon sequencing—so that both coarse- and fine-grained teleoperation modalities can be evaluated in
            an appropriately graduated setting.
          </p>
          <div style="text-align: center;">
            <img alt="task" src="./statics/images/task.png" width="100%" />
          </div>
          <p>
          <h3 class="title is-3 has-text-centered is-size-4-mobile">3.Observation</h3>
          </p>
          <p>
            Our simulation framework records a rich set of observations for every teleoperation episode, enabling
            downstream imitation-learning studies and further amplifying the value of TeleOpBench. The logged data
            include:
          </p>
          <ol>
            <li>
              robot-state vectors—joint positions, angles, velocities, and related kinematics;
            </li>
            <li>
              camera streams—RGB images from a head-mounted, first-person camera and a fixed third-person camera facing
              the workbench;
            </li>
            <li>
              task-level environment metadata—precise object positions and orientations.
            </li> -->
          <!-- </ol> -->
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<!-- 闭合 Task Environments -->


<!-- 加入真机视频模块 -->
<!-- 真机视频模块标题 -->
<h2 style="text-align: center; margin-top: 40px;">Real-World Robot Control with Four Teleoperation Modes</h2>

<!-- 通用视频行样式 -->
<style>
  .video-row {
    display: flex;
    justify-content: center;
    gap: 20px;
    margin-bottom: 30px;
    flex-wrap: wrap;
  }

  .video-box {
    max-width: 400px;
    flex: 1 1 45%;
    text-align: center;
  }

  .video-box video {
    width: 100%;
    border-radius: 10px;
  }
</style>

<!-- Vision -->
<h3 style="text-align: center;">Vision</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Vision1.mp4" type="video/mp4">
    </video>
    <p>Vision Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Vision2.mp4" type="video/mp4">
    </video>
    <p>Vision Control 2</p>
  </div>
</div>

<!-- VisionPro -->
<h3 style="text-align: center;">VisionPro</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Visionpro1.mp4" type="video/mp4">
    </video>
    <p>VisionPro Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Visionpro2.mp4" type="video/mp4">
    </video>
    <p>VisionPro Control 2</p>
  </div>
</div>

<!-- Exoskeleton -->
<h3 style="text-align: center;">Exoskeleton</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Exo1.mp4" type="video/mp4">
    </video>
    <p>Exoskeleton Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Exo2.mp4" type="video/mp4">
    </video>
    <p>Exoskeleton Control 2</p>
  </div>
</div>

<!-- Xsens -->
<h3 style="text-align: center;">Xsens</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Xsens1.mp4" type="video/mp4">
    </video>
    <p>Xsens Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Xsens2.mp4" type="video/mp4">
    </video>
    <p>Xsens Control 2</p>
  </div>
</div>


<!-- 仿真视频 -->
<!-- 模拟环境视频模块标题 -->
<h2 style="text-align: center; margin-top: 40px;">Simulated Control with Four Teleoperation Modes</h2>

<!-- 通用视频行样式（若前面已定义可省略） -->
<style>
  .video-row {
    display: flex;
    justify-content: center;
    gap: 20px;
    margin-bottom: 30px;
    flex-wrap: wrap;
  }

  .video-box {
    max-width: 400px;
    flex: 1 1 45%;
    text-align: center;
  }

  .video-box video {
    width: 100%;
    border-radius: 10px;
  }
</style>

<!-- Sim Vision -->
<h3 style="text-align: center;">Sim - Vision</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Vision1.mp4" type="video/mp4">
    </video>
    <p>Sim Vision Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Vision2.mp4" type="video/mp4">
    </video>
    <p>Sim Vision Control 2</p>
  </div>
</div>

<!-- Sim VisionPro -->
<h3 style="text-align: center;">Sim - VisionPro</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Visionpro1.mp4" type="video/mp4">
    </video>
    <p>Sim VisionPro Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Visionpro2.mp4" type="video/mp4">
    </video>
    <p>Sim VisionPro Control 2</p>
  </div>
</div>

<!-- Sim Exoskeleton -->
<h3 style="text-align: center;">Sim - Exoskeleton</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Exo1.mp4" type="video/mp4">
    </video>
    <p>Sim Exoskeleton Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Exo2.mp4" type="video/mp4">
    </video>
    <p>Sim Exoskeleton Control 2</p>
  </div>
</div>

<!-- Sim Xsens -->
<h3 style="text-align: center;">Sim - Xsens</h3>
<div class="video-row">
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Xsens1.mp4" type="video/mp4">
    </video>
    <p>Sim Xsens Control 1</p>
  </div>
  <div class="video-box">
    <video autoplay muted loop playsinline controls>
      <source src="./statics/videolq/Sim_Xsens2.mp4" type="video/mp4">
    </video>
    <p>Sim Xsens Control 2</p>
  </div>
</div>

<!-- 折线图的放入 -->
<h2 style="text-align: center; margin-top: 40px;">Sim and Real Deployment</h2>

<div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
  <div style="flex: 1; text-align: center; max-width: 30%;">
    <img src="./statics/teleop_images/sim.png" alt="Sim" style="width: 100%; height: auto;" />
    <p>Sim</p>
  </div>
  <div style="flex: 1; text-align: center; max-width: 30%;">
    <img src="./statics/teleop_images/real.png" alt="Real" style="width: 100%; height: auto;" />
    <p>Real</p>
  </div>
</div>


<h2 style="text-align: center; margin-top: 40px;">High-Precision Teleoperation Demonstration - Xsens</h2>

<!-- 视频1 -->
<div style="text-align: center; margin-bottom: 20px;">
  <video autoplay muted loop playsinline controls style="width: 60%; max-width: 800px;">
    <source src="./statics/videolq/High-Precision1.mp4" type="video/mp4">
    High-Precision Video 1
  </video>
  <p>High-Precision Control with Xsens - Demo 1</p>
</div>

<!-- 视频2 -->
<div style="text-align: center; margin-bottom: 20px;">
  <video autoplay muted loop playsinline controls style="width: 60%; max-width: 800px;">
    <source src="./statics/videolq/High-Precision2.mp4" type="video/mp4">
    High-Precision Video 2
  </video>
  <p>High-Precision Control with Xsens - Demo 2</p>
</div>

<!-- 仿真视频 -->
<div style="text-align: center; margin-bottom: 20px;">
  <video autoplay muted loop playsinline controls style="width: 60%; max-width: 800px;">
    <source src="./statics/videolq/Sim_High-Precision.mp4" type="video/mp4">
    Simulated High-Precision Xsens
  </video>
  <p>Simulated Xsens-Controlled Precision Task</p>
</div>





<section class="section is-small" id="Teleoperation">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered is-size-4-mobile">Teleoperation Interface</h2>
        <div class="content">
          <p>
            We implement four representative teleoperation pipelines—monocular vision, MoCap, VR, and exoskeleton— under
            a unified, modular interface.
          </p>


          <!-- <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 95%; height: 100%;">
                <source src="./statics/videolq/long.mp4" type="video/mp4">
                Walking under changing upper-body poses.
              </video>
              <p><span style="color: #126BAE;">1. </span>Walking under changing upper-body poses.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 95%; height: 100%;">
                <source src="./statics/videolq/squat.mp4" type="video/mp4">
                Squatting under changing upper-body poses.
              </video>
              <p><span style="color: #126BAE;">2. </span>Squatting under changing upper-body poses.</p>
            </div>
          </div>
          <br>
          <div style="text-align: center;">
            <img alt="vsopentv" src="./statics/images/opentv.png" width="80%" />
          </div>
          <p>The completion times for these tasks are computed based on data from three different operators, with each
            operator performing the tasks three times. Our hardware system can accelerate the teleoperation by
            approximately <span style="color: #e9890c;">2 times</span>, particularly in tasks that require radial
            movement.</p>
        </div> -->

          <div class="content">
            <h4 class="title is-3 has-text-centered is-size-4-mobile">1.Vision-based</h4>
            <p>
              The system first calibrates once in a neutral T‑pose, using SMPL to derive body shape β and link-scale
              factors s that align human and robot kinematics. During operation, SMPLer-X streams the operator's
              upper-body pose, which is rescaled by s and solved with PINK IK for arm-and-wrist motion, while MediaPipe
              key-points refined by Dex-Retargeting drive precise finger control. Decoupling limb and hand estimation
              yields robust, real-time teleoperation from pure vision input.
            </p>
            <h4 class="title is-3 has-text-centered is-size-4-mobile">2.MoCap-based</h4>
            <p>
              The inertial-MoCap pipeline uses an Xsens MVN suit (23 IMUs) plus Manus Metagloves. After a one-time
              calibration, the MVN stream provides the 6-DoF pose of 23 body segments, while each glove outputs 20
              finger-joint DoFs. Raw limb data are first transformed from the MVN's global frame to the robot frame
              (pelvis origin, forward +X, vertical +Z).
              <br>
              A joint-specific, real-time rescaling module then compensates for
              human-robot link-length mismatches before Closed-Loop Inverse Kinematics (CLIK) solves the robot's
              arm-and-wrist poses. For the hands, the glove's MCP, PIP, DIP and ab/adduction angles are mapped
              directly—subject to the dexterous hand's joint limits—yielding accurate, low-latency replication of both
              limb and finger motions.
            </p>
            <div style="text-align: center;">
              <img alt="xsens pipeline" src="./statics/images/Xsens_Pipe.png" width="100%" />
            </div>
            <h4 class="title is-3 has-text-centered is-size-4-mobile">3.VR-based</h4>
            <p>The <strong>VR-based teleoperation system</strong> includes two main components:</p>

            <h5>1. Upper-body Limb Motion Control</h5>
            <p>
              For upper-body limb motion control, the Apple VisionPro is utilized for hand, wrist, and head tracking,
              adhering to the OpenXR coordinate system. Wrist and head poses are initially transformed into the robot's
              coordinate frame. The wrist offset relative to the head is then converted into an offset relative to the
              pelvis. Only the wrist translation data is fed to an IK algorithm based on Pink, which computes all
              degrees of freedom except for finger joints.
            </p>
            <h5>2. Hand Control</h5>
            <p>
              For hand control, to enhance manual dexterity across different teleoperators, the distal phalanx lengths
              of
              each operator's fingers are measured and scaled proportionally to match the corresponding robotic finger
              segments. Subsequently, vector-based optimizers are employed, following the OpenTelevision approach, to
              generate robot-hand joint commands within the dexterous-retargeting framework of AnyTeleop.
            </p>
            <h5 class="title is-3 has-text-centered is-size-4-mobile">4.Exoskeleton-based</h5>
            <p>
              This exoskeleton-based teleoperation framework creates isomorphic systems customized to replicate a target
              humanoid's upper body kinematics, based on HOMIE principles. Servo-driven joints ensure real-time
              synchronization of operator and robot movements. Integrated motion-sensing gloves with Hall-effect sensors
              provide 15 DoF per-hand tracking. By directly mapping operator kinematics to the humanoid's joints, this
              method bypasses inverse kinematics (IK) approximations, thereby eliminating algorithmic errors and
              enhancing
              operational bandwidth and positional accuracy.
            </p>
            <div class="video-container" style="display: flex;">
              <div style="flex: 1; text-align: center;">
                <img alt="exo-g1" src="./statics/images/g1real.png" width="50%" />
                <p>
                  <span style="color: #126BAE;">1. </span>Isomorphic Exoskeleton for Unitree G1.
                </p>

              </div>
              <div style="flex: 1; text-align: center;">
                <img alt="exo-gr1" src="./statics/images/gr1real.png" width="50%" />
                <p><span style="color: #126BAE;">2. </span>Isomorphic Exoskeleton for Fourier GR-1.</p>

              </div>
              <div style="flex: 1; text-align: center;">
                <img alt="exo-h1" src="./statics/images/h1real.jpg" width="50%" />
                <p><span style="color: #126BAE;">3. </span>Isomorphic Exoskeleton for Unitree H1-2.</p>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
</section>
<!-- 闭合 Teleoperation -->

<section class="section is-small" id="Experiments">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-centered is-size-4-mobile">Experiments</h2>
        <div class="content">
          <p>
            From top to bottom, we illustrate the four teleoperation modalities executing the following tasks: ball
            trashcan, pen brushpot, ball bimanual, and pot bimanual.
          </p>
          <div style="text-align: center;">
            <img alt="bench all" src="./statics/images/Figure3_process.png" width="100%" />
          </div>

          <h4 class="title is-3 has-text-centered is-size-4-mobile">1. Simulation Results</h4>
          <p>We transfer the trained policies for Unitree G1 and Fourier GR-1 from Isaac Gym to scenes developed by <a
              href="https://github.com/OpenRobotLab/GRUtopia">GRUtopia</a>, thus the robots can perform diverse
            loco-manipulation tasks more cost-effectively and in a wider range of scenarios than would be feasible in
            the real world. </p>
          <!-- <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 90%; height: 100%;">
                <source src="./statics/videolq/g1sim.mp4" type="video/mp4">
                Unitree G1 in GRUtopia.
              </video>
              <p><span style="color: #126BAE;">1. </span>Unitree G1 in GRUtopia.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 90%; height: 100%;">
                <source src="./statics/videolq/gr1sim.mp4" type="video/mp4">
                Fourier GR-1 in GRUtopia.
              </video>
              <p><span style="color: #126BAE;">2. </span>Fourier GR-1 in GRUtopia.</p>
            </div>
          </div>
          <br>
          <br>
          <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 95%; height: 100%;">
                <source src="./statics/videolq/g1tasksim.mp4" type="video/mp4">
                Loco-Manipulation Task Completion in GRUtopia.
              </video>
              <p><span style="color: #126BAE;">3. </span>Loco-Manipulation Task Completion in GRUtopia.</p>
            </div>
          </div> -->
        </div>

        <div class="content">
          <p>
          <h4 class="title is-3 has-text-centered is-size-4-mobile">2. Real-world Results</h4>
          </p>
          <p>To validate the effectiveness of the demonstratons collected by HOMIE for IL algorithms, we design two
            distinct tasks, collect data by teleoperating, train with IL algorithm, and deploy in the real world. We
            achieve over <span style="color: #e9890c;">70%</span> success rate, showing the feasibility of training IL
            with collected data.</p>
          <!-- <div class="video-container" style="display: flex;">
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 90%; height: 100%;">
                <source src="./statics/videolq/imi.mp4" type="video/mp4">
                Squat Pick.
              </video>
              <p><span style="color: #126BAE;">1. </span>Squat Pick.</p>
            </div>
            <div style="flex: 1; text-align: center;">
              <video  autoplay muted loop playsinline controls  style="width: 90%; height: 100%;">
                <source src="./statics/videolq/imi2.mp4" type="video/mp4">
                Pick & Place.
              </video>
              <p><span style="color: #126BAE;">2. </span>Pick & Place.</p>
            </div>
          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 is-size-4-mobile">Authors</h2>
    <div class="container has-text-centered">
      <div class="publication-authors is-flex is-flex-wrap-wrap is-justify-content-space-around">

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://scholar.google.com/citations?user=gf-TicYAAAAJ&hl=zh-CN">
              <img class="is-rounded" src="./statics/authors/hangyu.jpeg" alt="Hangyu Li">
            </a>
          </figure>
          <a href="https://scholar.google.com/citations?user=gf-TicYAAAAJ&hl=zh-CN">Hangyu Li</a><sup>1,4,*</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://scholar.google.com/citations?user=c4OKn6IAAAAJ">
              <img class="is-rounded" src="./statics/authors/qin.jpg" alt="Qin Zhao">
            </a>
          </figure>
          <a href="https://scholar.google.com/citations?user=c4OKn6IAAAAJ">Qin Zhao</a><sup>1,2,*</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="">
              <img class="is-rounded" src="./statics/authors/haoran.jpeg" alt="Haoran Xu">
            </a>
          </figure>
          <a href="">Haoran Xu</a><sup>1,2</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=njfKRXQAAAAJ">
              <img class="is-rounded" src="./statics/authors/xinyu.png" alt="Xinyu Jiang">
            </a>
          </figure>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=njfKRXQAAAAJ">Xinyu Jiang</a><sup>1</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://www.qingweiben.com/">
              <img class="is-rounded" src="./statics/authors/qingwei.jpeg" alt="Qingwei Ben">
            </a>
          </figure>
          <a href="https://www.qingweiben.com/">Qingwei Ben</a><sup>1,3</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="">
              <img class="is-rounded" src="./statics/authors/feiyu.png" alt="Feiyu Jia">
            </a>
          </figure>
          <a href="https://trap-1.github.io/">Feiyu Jia</a><sup>1</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://maxwell-zhao.github.io/">
              <img class="is-rounded" src="./statics/authors/haoyu.png" alt="Haoyu Zhao">
            </a>
          </figure>
          <a href="https://maxwell-zhao.github.io/">Haoyu Zhang</a><sup>1</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://liangxuy.github.io/">
              <img class="is-rounded" src="./statics/authors/liangxu.png" alt="Liang Xu">
            </a>
          </figure>
          <a href="https://liangxuy.github.io/">Liang Xu</a><sup>1</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://scholar.google.com/citations?user=kYrUfMoAAAAJ&hl=zh-CN">
              <img class="is-rounded" src="./statics/authors/jiazeng.jpeg" alt="Jia Zeng">
            </a>
          </figure>
          <a href="https://scholar.google.com/citations?user=kYrUfMoAAAAJ&hl=zh-CN">Jia Zeng</a><sup>1</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="">
              <img class="is-rounded" src="./statics/authors/hanqing.png" alt="Hanqing Wang">
            </a>
          </figure>
          <a href="https://hanqingwangai.github.io/">Hanqing Wang</a><sup>1</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x256">
            <a href="https://daibo.info/">
              <img class="is-rounded" src="./statics/authors/bodai.jpeg" alt="Bo Dai">
            </a>
          </figure>
          <a href="https://daibo.info/">Bo Dai</a><sup>5,6</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://jtdong.com/">
              <img class="is-rounded" src="./statics/authors/junting.jpg" alt="Junting Dong">
            </a>
          </figure>
          <a href="https://jtdong.com/">Junting Dong</a><sup>1,&dagger;</sup>
        </div>

        <div class="author-block has-text-centered has-addons-centered">
          <figure class="image is-128x128">
            <a href="https://oceanpang.github.io/">
              <img class="is-rounded" src="./statics/authors/miao.png" alt="Jiangmiao Pang">
            </a>
          </figure>
          <a href="https://oceanpang.github.io/">Jiangmiao Pang</a><sup>1</sup>
        </div>
      </div>

      <div class="publication-authors">
        <span class="author-block"><sup>1</sup>Shanghai Artificial Intelligence Laboratory, </span>
        <span class="author-block"><sup>2</sup>Zhejiang University, </span>
        <span class="author-block"><sup>3</sup>The Chinese University of Hong Kong, </span>
        <br>
        <span class="author-block"><sup>4</sup>The Hong Kong University of Science and Technology (Guangzhou), </span>
        <br>
        <span class="author-block"><sup>5</sup>The University of Hong Kong, </span>
        <span class="author-block"><sup>6</sup>Feeling AI</span>
        <br>
        <span class="author-block">*Equal contribution</span>
        <span class="author-block">&dagger;Corresponding author</span>
      </div>

    </div>
    <br>
    <!-- <pre class="is-hidden-mobile"><code>@article{ben2024homie,
  title={TeleOpBench: A Simulator-Centric Benchmark for
Dual-Arm Dexterous Teleoperation},
  author={Qingwei Ben, Feiyu Jia, Jia Zeng, Junting Dong, Dahua Lin, Jiangmiao Pang},
  journal={arXiv preprint arXiv:2502.13013},
  year={2025}
}</code></pre> -->
    <p>
      If you have any questions, please contact <a href="mailto:cyjdlhy@gmail.com">Hangyu Li<sup>&#9993;</sup></a> and
      <a href="mailto:zhaoqin@pjab.org.cn">Qin Zhao<sup>&#9993;</sup></a>. 🎉
    </p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template modified from <a href="https://homietele.github.io/">HOMIE</a>, <a
              href="https://nerfies.github.io/">NeRFies</a> and <a href="https://umi-on-legs.github.io/">UMI on
              Legs</a>.
          </p>
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow and modify the <a
              href="https://github.com/nerfies/nerfies.github.io">source
              code</a> of this website as long as
            you link back to the <a href="https://nerfies.github.io/">NeRFies</a> page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>

</html>